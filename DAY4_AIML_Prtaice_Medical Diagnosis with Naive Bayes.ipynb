{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7cb0b2",
   "metadata": {},
   "source": [
    "# Medical Diagnosis with Naive Bayes\n",
    "\n",
    "You work for a medical research institute, and your task is to develop a diagnostic system using the Naive Bayes algorithm. You have a dataset with various medical test results, patient information, and corresponding diagnoses (e.g., presence or absence of a medical condition). Your goal is to create a classification model to aid in the medical diagnosis process. Answer the following questions based on this case study.\n",
    "\n",
    "1. Data Exploration:\n",
    "\n",
    "a. Load and explore the medical dataset using Python libraries like pandas. Describe the features, label, and the distribution of diagnoses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2188ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0  ...          17.33           184.60      2019.0            0.1622   \n",
      "1  ...          23.41           158.80      1956.0            0.1238   \n",
      "2  ...          25.53           152.50      1709.0            0.1444   \n",
      "3  ...          26.50            98.87       567.7            0.2098   \n",
      "4  ...          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "3             0.8663           0.6869                0.2575          0.6638   \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "3                  0.17300          NaN  \n",
      "4                  0.07678          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Features: Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst'],\n",
      "      dtype='object')\n",
      "Label: Unnamed: 32\n",
      "Diagnosis Distribution:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('C:/Users/hp/Documents/data_day4_medical.csv')  # Replace \"medical_data.csv\" with the actual filename or URL of your dataset\n",
    "\n",
    "# Display the first few rows of the dataset to understand the features\n",
    "print(data.head())\n",
    "\n",
    "# Describe the features\n",
    "features = data.columns[:-1]  # All columns except the last one (diagnosis)\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Describe the label (diagnosis)\n",
    "label = data.columns[-1]  # The last column\n",
    "print(\"Label:\", label)\n",
    "\n",
    "# Check the distribution of diagnoses\n",
    "diagnosis_distribution = data[label].value_counts()\n",
    "print(\"Diagnosis Distribution:\")\n",
    "print(diagnosis_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830e4ee",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing:\n",
    "\n",
    "a. Explain the necessary data preprocessing steps for preparing the medical data. This may include handling missing values, normalizing or scaling features, and encoding categorical variables.\n",
    "\n",
    "b. Calculate the prior probabilities P(Condition) and P(No Condition) based on the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152ea343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID Diagnosis  Mean Radius  Mean Texture  Mean Perimeter  Mean Area  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   Mean Smoothness  Mean Compactness  Mean Concavity  Mean Concave Points  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  Worst Radius  Worst Texture  Worst Perimeter  Worst Area  \\\n",
      "0  ...         25.38          17.33           184.60      2019.0   \n",
      "1  ...         24.99          23.41           158.80      1956.0   \n",
      "2  ...         23.57          25.53           152.50      1709.0   \n",
      "3  ...         14.91          26.50            98.87       567.7   \n",
      "4  ...         22.54          16.67           152.20      1575.0   \n",
      "\n",
      "   Worst Smoothness  Worst Compactness  Worst Concavity  Worst Concave Points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   Worst Symmetry  Worst Fractal Dimension  \n",
      "0          0.4601                  0.11890  \n",
      "1          0.2750                  0.08902  \n",
      "2          0.3613                  0.08758  \n",
      "3          0.6638                  0.17300  \n",
      "4          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "Diagnosis\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "print(data.head())  # Display the first few rows to understand the structure of the data\n",
    "print(data['Diagnosis'].value_counts())  # Check the class distribution of the 'diagnosis' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55614414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Condition): 0.37258347978910367\n",
      "P(No Condition): 0.6274165202108963\n"
     ]
    }
   ],
   "source": [
    "# Calculate the prior probabilities\n",
    "class_counts = data['Diagnosis'].value_counts()\n",
    "total_instances = len(data)\n",
    "\n",
    "# P(Condition) for 'M' (Malignant)\n",
    "P_condition = class_counts['M'] / total_instances\n",
    "\n",
    "# P(No Condition) for 'B' (Benign)\n",
    "P_no_condition = class_counts['B'] / total_instances\n",
    "\n",
    "print(\"P(Condition):\", P_condition)\n",
    "print(\"P(No Condition):\", P_no_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9dd6ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Diagnosis', 'Mean Radius', 'Mean Texture', 'Mean Perimeter',\n",
      "       'Mean Area', 'Mean Smoothness', 'Mean Compactness', 'Mean Concavity',\n",
      "       'Mean Concave Points', 'Mean Symmetry', 'Mean Fractal Dimension',\n",
      "       'SE Radius', 'SE Texture', 'SE Perimeter', 'SE Area', 'SE Smoothness',\n",
      "       'SE Compactness', 'SE Concavity', 'SE Concave Points', 'SE Symmetry',\n",
      "       'SE Fractal Dimension', 'Worst Radius', 'Worst Texture',\n",
      "       'Worst Perimeter', 'Worst Area', 'Worst Smoothness',\n",
      "       'Worst Compactness', 'Worst Concavity', 'Worst Concave Points',\n",
      "       'Worst Symmetry', 'Worst Fractal Dimension'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# List the column names in the dataset\n",
    "print(data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5758d266",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering:\n",
    "\n",
    "a. Describe how to convert the medical test results and patient information into suitable features for the Naive Bayes model.\n",
    "\n",
    "b. Discuss the importance of feature selection or dimensionality reduction in medical diagnosi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1de1a7",
   "metadata": {},
   "source": [
    "# 4. Implementing Naive Bayes:\n",
    "\n",
    "a. Choose the appropriate Naive Bayes variant (e.g., Gaussian, Multinomial, or Bernoulli Naive Bayes) for the medical diagnosis task and implement the classifier using Python libraries like scikit-learn.\n",
    "\n",
    "b. Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377b8189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.96      1.00      0.98        71\n",
      "           M       1.00      0.93      0.96        43\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin (Diagnostic) dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "column_names = [\"ID\", \"Diagnosis\", \"Mean Radius\", \"Mean Texture\", \"Mean Perimeter\", \"Mean Area\", \"Mean Smoothness\", \"Mean Compactness\", \"Mean Concavity\", \"Mean Concave Points\", \"Mean Symmetry\", \"Mean Fractal Dimension\", \"SE Radius\", \"SE Texture\", \"SE Perimeter\", \"SE Area\", \"SE Smoothness\", \"SE Compactness\", \"SE Concavity\", \"SE Concave Points\", \"SE Symmetry\", \"SE Fractal Dimension\", \"Worst Radius\", \"Worst Texture\", \"Worst Perimeter\", \"Worst Area\", \"Worst Smoothness\", \"Worst Compactness\", \"Worst Concavity\", \"Worst Concave Points\", \"Worst Symmetry\", \"Worst Fractal Dimension\"]\n",
    "data = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.iloc[:, 2:]  # Features start from column 2\n",
    "y = data[\"Diagnosis\"]  # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit the model on the training data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and generate a classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02a948",
   "metadata": {},
   "source": [
    "# 5. Model Training:\n",
    "\n",
    "a. Train the Naive Bayes model using the feature-engineered dataset. Explain the probability estimation process in Naive Bayes for medical diagnosis..\n",
    "\n",
    "dataset, which includes handling missing values, normalizing or scaling features, encoding categorical variables, and splitting the dataset into training and testing sets.\n",
    "\n",
    "Choose the Appropriate Naive Bayes Variant: In this case, you have chosen the Gaussian Naive Bayes variant because it's suitable for datasets with continuous features.\n",
    "\n",
    "Model Training: You use the training dataset to fit the Gaussian Naive Bayes model. The model learns the statistical properties of the features in each class (e.g., presence or absence of a medical condition).\n",
    "\n",
    "Probability Estimation: During the model training, the Gaussian Naive Bayes algorithm estimates two types of probabilities:\n",
    "\n",
    "Class Prior Probability (P(Condition) and P(No Condition)): These are the probabilities of a data point belonging to each class (e.g., having the medical condition or not). You calculated these in a previous step during data preprocessing.\n",
    "Feature Probability Distributions (P(X|Condition) and P(X|No Condition)): For each feature, the model estimates the probability distribution for each class. In Gaussian Naive Bayes, it assumes that the features follow a Gaussian (normal) distribution within each class. Therefore, it calculates the mean and variance of each feature for each class.\n",
    "Feature Independence Assumption: The \"naive\" assumption in Naive Bayes is that features are conditionally independent given the class. This means that the model assumes that knowing the value of one feature doesn't provide any information about the values of other features. This assumption simplifies the probability calculations.\n",
    "\n",
    "Posterior Probability Estimation: When you make predictions, the Naive Bayes algorithm calculates the posterior probability for each class given the observed values of the features. It uses Bayes' theorem to calculate this probability. The class with the highest posterior probability is predicted as the class label.\n",
    "\n",
    "Prediction: The model predicts the class label with the highest posterior probability.\n",
    "\n",
    "Model Evaluation: After training, you evaluate the model's performance on the testing dataset using appropriate metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "The key to Naive Bayes' success in medical diagnosis is its simplicity and efficiency in estimating probabilities. It works well when the features are conditionally independent or when the independence assumption is reasonable, even if it doesn't always hold in practice. It provides interpretable results and can be a valuable tool in the medical field for diagnostic purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b42ad",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation:\n",
    "\n",
    "a. Assess the performance of the medical diagnosis model using relevant evaluation metrics, such as accuracy, precision, recall, and F1-score..\n",
    "\n",
    "b. Interpret the results and discuss the model's ability to accurately classify medical conditions.\n",
    "a. Performance Evaluation Metrics:\n",
    "\n",
    "In medical diagnosis, it's crucial to assess the performance of the model accurately. Relevant evaluation metrics include:\n",
    "\n",
    "Accuracy: The percentage of correctly classified instances. It's a good overall measure of the model's performance but can be misleading if there's a class imbalance.\n",
    "\n",
    "Precision: The percentage of true positive predictions among all positive predictions. It measures the model's ability to avoid false positives. In a medical context, precision indicates the ability to avoid diagnosing a condition when it's not present.\n",
    "\n",
    "Recall (Sensitivity): The percentage of true positive predictions among all actual positive instances. It measures the model's ability to correctly identify positive cases. In medical diagnosis, recall indicates the ability to detect a condition when it's present.\n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall. It provides a balance between precision and recall. A higher F1-score indicates a better trade-off between false positives and false negatives.\n",
    "\n",
    "Confusion Matrix: This matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives. It's a valuable tool for understanding the model's performance, especially in the context of medical diagnosis.\n",
    "\n",
    "b. Interpreting the Results:\n",
    "\n",
    "Interpreting the results of a medical diagnosis model is critical, as the consequences of misclassification can be significant. Here's how you might interpret the results:\n",
    "\n",
    "Accuracy: High accuracy is desirable, but it should be considered alongside other metrics, especially if there is class imbalance. A high accuracy might mask problems if the model is biased toward the majority class.\n",
    "\n",
    "Precision: A high precision means the model is good at avoiding false positives. In medical diagnosis, this is crucial because it minimizes the chance of diagnosing a condition when it's not present, reducing unnecessary treatments or procedures.\n",
    "\n",
    "Recall (Sensitivity): High recall is important to ensure that the model correctly identifies true positive cases. In medical diagnosis, this means that the model is good at detecting the condition when it's present, reducing the risk of false negatives.\n",
    "\n",
    "F1-Score: The F1-score balances precision and recall. A high F1-score indicates a good trade-off between minimizing false positives and false negatives.\n",
    "\n",
    "Confusion Matrix: Examining the confusion matrix can provide insights into the model's specific strengths and weaknesses. For instance, it can reveal if the model has a particular tendency to produce more false positives or false negatives.\n",
    "\n",
    "Clinical Relevance: Interpret the results in the context of medical practice. Consider the clinical impact of false positives and false negatives. Depending on the disease, one type of error might be more critical than the other.\n",
    "\n",
    "Domain Expertise: Collaborate with domain experts, such as medical professionals, to understand the practical implications of the model's performance. They can provide insights into the clinical relevance of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c05ab",
   "metadata": {},
   "source": [
    "# 7. Laplace Smoothing:\n",
    "\n",
    "a. Explain the concept of Laplace (add-one) smoothing and discuss its potential application in the context of medical diagnosis.\n",
    "\n",
    "b. Discuss the impact of Laplace smoothing on model performance\n",
    "a. Concept of Laplace (Add-One) Smoothing:\n",
    "\n",
    "Laplace smoothing, also known as add-one smoothing or add-k smoothing, is a technique used in probability theory and statistics to address the problem of zero probabilities in categorical data. It's particularly useful in the context of Naive Bayes classification for medical diagnosis and other applications where you have discrete or categorical features.\n",
    "\n",
    "The basic idea behind Laplace smoothing is to add a small constant (usually 1) to the count of each category within a feature, as well as to the total number of categories. This ensures that no category has a zero probability, even if it has not been observed in the training data. In mathematical terms, Laplace smoothing is often applied to estimate conditional probabilities as follows:\n",
    "\n",
    "V is the number of unique categories in the feature.\n",
    "In the context of medical diagnosis, Laplace smoothing is beneficial because it helps avoid zero probabilities for specific combinations of symptoms or test results that may not occur in the training dataset. This can be especially important when working with medical data, as it ensures that the model doesn't completely rule out a potential diagnosis due to lack of observed instances.\n",
    "\n",
    "b. Impact of Laplace Smoothing on Model Performance:\n",
    "\n",
    "The impact of Laplace smoothing on model performance in medical diagnosis can be both positive and negative, depending on the data and the choice of the smoothing constant. Here are some considerations:\n",
    "\n",
    "Positive Impact:\n",
    "\n",
    "Reducing Overfitting: Laplace smoothing can help reduce the risk of overfitting, especially in cases where you have limited data. It makes the model more robust and less sensitive to rare events.\n",
    "\n",
    "Avoiding Zero Probabilities: Laplace smoothing ensures that no category has a zero probability, which is essential for the Naive Bayes algorithm's calculations. It allows the model to make predictions for previously unseen combinations of features.\n",
    "\n",
    "Improved Generalization: Smoothing helps the model generalize better, making it more suitable for real-world applications where medical conditions may manifest in various ways.\n",
    "\n",
    "Negative Impact:\n",
    "\n",
    "Bias: Excessive smoothing (choosing a large constant, such as 1) can introduce bias into the model. This bias can make the model less accurate, particularly when there is a substantial amount of training data.\n",
    "\n",
    "Sensitivity to the Smoothing Constant: The choice of the smoothing constant can impact the model's performance. It may require experimentation to find the optimal value that balances smoothing and model accuracy.\n",
    "\n",
    "In practice, the choice of whether to use Laplace smoothing and the selection of the smoothing constant depend on the specific dataset and the trade-off between preventing zero probabilities and introducing bias. It's important to evaluate the model's performance with and without smoothing to determine its effectiveness in a medical diagnosis application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ecda9a",
   "metadata": {},
   "source": [
    "# Customer Segmentation with K-Nearest Neighbors (KNN)\n",
    "\n",
    "You work for a retail company, and your task is to segment customers based on their purchase behavior using the K-Nearest Neighbors (KNN) algorithm. The dataset contains information about customers, such as purchase history, age, and income. Your goal is to create customer segments for targeted marketing Answer the following questions based on this case study:\n",
    "\n",
    "1. Data Exploration:\n",
    "\n",
    "a. Load the customer dataset using Python libraries like pandas and explore its structure. Describe the features, target variable, and data distribution.\n",
    "\n",
    "b. Explain the importance of customer segmentation in the retail industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3f9c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0           1    Male   19                  15                      39\n",
      "1           2    Male   21                  15                      81\n",
      "2           3  Female   20                  16                       6\n",
      "3           4  Female   23                  16                      77\n",
      "4           5  Female   31                  17                      40\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   CustomerID              200 non-null    int64 \n",
      " 1   Gender                  200 non-null    object\n",
      " 2   Age                     200 non-null    int64 \n",
      " 3   Annual Income (k$)      200 non-null    int64 \n",
      " 4   Spending Score (1-100)  200 non-null    int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 7.9+ KB\n",
      "None\n",
      "       CustomerID         Age  Annual Income (k$)  Spending Score (1-100)\n",
      "count  200.000000  200.000000          200.000000              200.000000\n",
      "mean   100.500000   38.850000           60.560000               50.200000\n",
      "std     57.879185   13.969007           26.264721               25.823522\n",
      "min      1.000000   18.000000           15.000000                1.000000\n",
      "25%     50.750000   28.750000           41.500000               34.750000\n",
      "50%    100.500000   36.000000           61.500000               50.000000\n",
      "75%    150.250000   49.000000           78.000000               73.000000\n",
      "max    200.000000   70.000000          137.000000               99.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the customer dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "data = pd.read_csv(\"C:/Users/hp/Documents/Mall_Customers.csv\")\n",
    "\n",
    "# Display the first few rows to understand the dataset's structure\n",
    "print(data.head())\n",
    "\n",
    "# Get information about the dataset, including data types and missing values\n",
    "print(data.info())\n",
    "\n",
    "# Describe the statistical summary of numerical features\n",
    "print(data.describe())\n",
    "\n",
    "# Explore the distribution of categorical variables (if any)\n",
    "# Replace 'categorical_column' with the actual name of the categorical column\n",
    "if 'categorical_column' in data.columns:\n",
    "    print(data['categorical_column'].value_counts())\n",
    "\n",
    "# Identify the target variable (if available) and describe its distribution\n",
    "# Replace 'target_variable' with the actual name of the target variable\n",
    "if 'target_variable' in data.columns:\n",
    "    print(data['target_variable'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. implementing KNN:\n",
    "\n",
    "a. Implement the K-Nearest Neighbors algorithm using Python libraries like scikit-learn to segment customers based on their features.\n",
    "\n",
    "b. Choose an appropriate number of neighbors (K) for the algorithm and explain your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06d88146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.96      0.96      0.96        71\n",
      "           M       0.93      0.93      0.93        43\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming 'X' contains the feature variables and 'y' contains the target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature variables\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You need to choose an appropriate value for 'n_neighbors'\n",
    "\n",
    "# Train the KNN classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Model Training:\n",
    "\n",
    "a. Train the KNN model using the preprocessed customer dataset.\n",
    "\n",
    "b. Discuss the distance metric used for finding the nearest neighbors and its significance in customer segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5781bae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You need to choose an appropriate value for 'n_neighbors'\n",
    "\n",
    "# Train the KNN classifier\n",
    "knn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cadecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Customer Segmentation:\n",
    "\n",
    "a. Segment the customers based on their purchase behavior, age, and income.\n",
    "b. Visualize the customer segments to gain insights into the distribution and characteristics of each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "222ecaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a))))))))ssuming 'X' contains the feature variables for customer data\n",
    "# Replace 'X' with the actual feature data\n",
    "\n",
    "# Standardize the feature variables (if not already done during preprocessing)\n",
    "X_standardized = scaler.transform(X)  # Use the same scaler used for training\n",
    "\n",
    "# Predict customer segments using the trained KNN model\n",
    "customer_segments = knn.predict(X_standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9912389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (569) does not match length of index (200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Add the cluster labels to your data\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3949\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4135\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4136\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4143\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4146\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4147\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4148\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4149\u001b[0m     ):\n\u001b[0;32m   4150\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4870\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4870\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (569) does not match length of index (200)"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming 'X' contains the feature variables\n",
    "# Replace 'X' with your actual feature data\n",
    "\n",
    "# Create a K-Means model with the chosen number of clusters (e.g., K=3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Add the cluster labels to your data\n",
    "data['Cluster'] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86e4a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n",
      "['M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M']\n"
     ]
    }
   ],
   "source": [
    "print(len(customer_segments))\n",
    "print(customer_segments[:10])  # Print the first 10 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    " Hyperparameter Tuning:\n",
    "\n",
    "\n",
    "\n",
    "b. Conduct hyperparameter tuning for the KNN model and discuss the impact of different values of K on segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9dad777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=13)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=13)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=13)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a range of 'K' values to search\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Find the best 'K' value\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Train the KNN model with the best 'K' value\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "best_knn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0a4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
